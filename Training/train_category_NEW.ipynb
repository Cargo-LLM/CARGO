{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge, LogisticRegression, RidgeClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# --- Category mapping ---\n",
    "category_map = {\n",
    "    'Summaries': ['summaries'],\n",
    "    'English': ['english_1', 'english_2'],\n",
    "    'Math': ['math_1', 'math_2'],\n",
    "    'Coding': ['coding_1', 'coding_2', 'coding_3'],\n",
    "    'Reasoning': ['reasoning_1', 'reasoning_2', 'reasoning_3']\n",
    "}\n",
    "llm_names = [\"openai/gpt-4o\", \"anthropic/claude-3.5-sonnet\", \"deepseek/deepseek-chat\", \"perplexity/sonar\"]\n",
    "\n",
    "# --- Load Data ---\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]].drop_duplicates().set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "# --- FIX: Ensure 'scores' is always a list of floats in the correct LLM order ---\n",
    "def scores_to_list(val):\n",
    "    # Always return a list of floats in the right LLM order\n",
    "    if isinstance(val, dict):\n",
    "        return [val.get(name, 0.0) for name in llm_names]\n",
    "    if isinstance(val, (list, np.ndarray)):\n",
    "        if len(val) == len(llm_names):\n",
    "            return list(val)\n",
    "        else:\n",
    "            return [0.0] * len(llm_names)\n",
    "    return [0.0] * len(llm_names)\n",
    "\n",
    "df_processed[\"scores\"] = df_processed[\"scores\"].apply(scores_to_list)\n",
    "\n",
    "# --- Models to Evaluate ---\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(RandomForestRegressor(n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1)),\n",
    "    \"XGBoost\": MultiOutputRegressor(XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, objective=\"reg:squarederror\", random_state=42, n_jobs=-1)),\n",
    "    \"MLP\": MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(256, 128), activation='relu', solver='adam', max_iter=300, random_state=42)),\n",
    "    \"Ridge\": MultiOutputRegressor(Ridge(alpha=1.0)),\n",
    "    \"SVR\": MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "}\n",
    "\n",
    "classifier_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=150, max_depth=25, random_state=42, n_jobs=-1),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=250, multi_class=\"ovr\"),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=150, max_depth=6, use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=250, random_state=42),\n",
    "    \"Ridge\": RidgeClassifier()\n",
    "}\n",
    "\n",
    "# --- Evaluation Loop ---\n",
    "for category, sources in category_map.items():\n",
    "    print(f\"\\n===== Category: {category} =====\")\n",
    "\n",
    "    # Filter category data\n",
    "    df_cat = df_processed[df_processed[\"Source\"].isin(sources)]\n",
    "    n_samples = len(df_cat)\n",
    "    if n_samples < 10:\n",
    "        print(f\"  [!] Not enough data for category {category} ({n_samples} samples), skipping.\")\n",
    "        continue\n",
    "\n",
    "    X = np.vstack(df_cat[\"embedding\"].values)\n",
    "    y_scores = np.array(df_cat[\"scores\"].tolist())\n",
    "    if y_scores.ndim == 1:\n",
    "        y_scores = y_scores.reshape(1, -1)\n",
    "    y_best = np.argmax(y_scores, axis=1)  # Best LLM index for each example\n",
    "\n",
    "    # Normalize X\n",
    "    X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test, yb_train, yb_test = train_test_split(\n",
    "        X_norm, y_scores, y_best, test_size=0.2, random_state=42, stratify=y_best\n",
    "    )\n",
    "\n",
    "    print(\"---- Regression Models ----\")\n",
    "    for name, model in regression_models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Per-LLM MSE\n",
    "        mse_per_llm = [\n",
    "            mean_squared_error(y_test[:, i], y_pred[:, i])\n",
    "            for i in range(y_scores.shape[1])\n",
    "        ]\n",
    "\n",
    "        # Top-1 ACC, Top-1-or-2 ACC\n",
    "        true_best = np.argmax(y_test, axis=1)\n",
    "        # For true second-best, sort descending, then pick index 1\n",
    "        sorted_idxs = np.argsort(y_test, axis=1)  # ascending\n",
    "        true_2nd = sorted_idxs[:, -2]\n",
    "        pred_best = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        top1_acc = np.mean(pred_best == true_best)\n",
    "        top1or2_acc = np.mean([\n",
    "            pred_best[i] in {true_best[i], true_2nd[i]}\n",
    "            for i in range(len(pred_best))\n",
    "        ])\n",
    "\n",
    "        print(f\"  {name:>10}: Per-LLM MSE={np.round(mse_per_llm, 4)}, Top-1 Acc={top1_acc:.3f}, Top-1-or-2 Acc={top1or2_acc:.3f}\")\n",
    "\n",
    "    # ---- Classification Models ----\n",
    "    print(\"---- Classifiers for Best LLM ----\")\n",
    "    # Convert yb_train/yb_test to categorical indices; need one-hot for multiclass AUC\n",
    "    yb_train_bin = label_binarize(yb_train, classes=np.arange(len(llm_names)))\n",
    "    yb_test_bin = label_binarize(yb_test, classes=np.arange(len(llm_names)))\n",
    "\n",
    "    for name, clf in classifier_models.items():\n",
    "        clf.fit(X_train, yb_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_prob = clf.predict_proba(X_test) if hasattr(clf, \"predict_proba\") else None\n",
    "\n",
    "        precision = precision_score(yb_test, y_pred, average='macro')\n",
    "        recall = recall_score(yb_test, y_pred, average='macro')\n",
    "        f1 = f1_score(yb_test, y_pred, average='macro')\n",
    "        acc = accuracy_score(yb_test, y_pred)\n",
    "\n",
    "        # Macro-AUC (One-vs-Rest), if probabilities available\n",
    "        if y_prob is not None:\n",
    "            auc = roc_auc_score(yb_test_bin, y_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "            print(f\"  {name:>18}: Acc={acc:.3f}, Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {name:>18}: Acc={acc:.3f}, Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, AUC=N/A\")\n",
    "\n",
    "print(\"\\n[Done]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Load and prepare the data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "llm_keys = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"perplexity/sonar\"\n",
    "]\n",
    "y_scores_df = pd.DataFrame(df_processed[\"scores\"].tolist(), columns=llm_keys)\n",
    "Y = y_scores_df.values\n",
    "\n",
    "# Macro-category mapping\n",
    "category_map = {\n",
    "    'Summaries': ['summaries'],\n",
    "    'English': ['english_1', 'english_2'],\n",
    "    'Math': ['math_1', 'math_2'],\n",
    "    'Coding': ['coding_1', 'coding_2', 'coding_3'],\n",
    "    'Reasoning': ['reasoning_1', 'reasoning_2', 'reasoning_3']\n",
    "}\n",
    "\n",
    "# For stratified split, create macro-category labels\n",
    "macro_labels = []\n",
    "macro_names = []\n",
    "for macro_cat, subcats in category_map.items():\n",
    "    macro_names.append(macro_cat)\n",
    "    macro_labels.extend([macro_cat] * sum(df_processed[\"Source\"].isin(subcats)))\n",
    "\n",
    "df_processed[\"Macro\"] = None\n",
    "for macro_cat, subcats in category_map.items():\n",
    "    df_processed.loc[df_processed[\"Source\"].isin(subcats), \"Macro\"] = macro_cat\n",
    "macro_label_array = df_processed[\"Macro\"].values\n",
    "\n",
    "# Split once for global router/classifier\n",
    "X_train, X_test, Y_train, Y_test, macro_train, macro_test = train_test_split(\n",
    "    X_norm, Y, macro_label_array, test_size=0.20, stratify=macro_label_array, random_state=42\n",
    ")\n",
    "\n",
    "# Train regression router (global)\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1\n",
    "))\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# === Load or train global MLP binary classifier ===\n",
    "with open(\"df_pairwise_v2.pkl\", \"rb\") as f:\n",
    "    df_pairwise = pickle.load(f)\n",
    "X_embed_cls = np.vstack(df_pairwise[\"embedding\"].values)\n",
    "encoder_cls = OneHotEncoder(sparse_output=False)\n",
    "X_cat_cls = encoder_cls.fit_transform(df_pairwise[[\"llm_A\", \"llm_B\"]])\n",
    "X_cls = np.hstack([X_embed_cls, X_cat_cls])\n",
    "y_cls = df_pairwise[\"label\"].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_cls, X_val_cls, y_train_cls, y_val_cls = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=42\n",
    ")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "key_to_short = {\n",
    "    \"openai/gpt-4o\": \"gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\": \"claude\",\n",
    "    \"deepseek/deepseek-chat\": \"deepseek\",\n",
    "    \"perplexity/sonar\": \"perplexity\"\n",
    "}\n",
    "\n",
    "# === Per macro-category evaluation ===\n",
    "thresholds = np.arange(0.01, 0.21, 0.01)\n",
    "category_results = {}\n",
    "\n",
    "for macro_cat, subcats in category_map.items():\n",
    "    idx = [i for i, c in enumerate(macro_test) if c == macro_cat]\n",
    "    if len(idx) == 0:\n",
    "        print(f\"[!] No test samples for macro-category {macro_cat}, skipping.\")\n",
    "        continue\n",
    "    print(f\"\\n=== Macro-category: {macro_cat} ({len(idx)} samples) ===\")\n",
    "    routed_accuracies = []\n",
    "    selection_accuracies = []\n",
    "    for tau in thresholds:\n",
    "        routed_set_contains_best = []\n",
    "        overall_selection_correct = []\n",
    "\n",
    "        for j in idx:\n",
    "            emb = X_test[j]\n",
    "            y_pred = Y_pred[j]\n",
    "            true_best = np.argmax(Y_test[j])\n",
    "            sorted_preds = np.sort(y_pred)[::-1]\n",
    "            ranked_preds = np.argsort(y_pred)[::-1]\n",
    "            top1_idx = ranked_preds[0]\n",
    "            top2_idx = ranked_preds[1]\n",
    "            top1_score = sorted_preds[0]\n",
    "            top2_score = sorted_preds[1]\n",
    "            gap = top1_score - top2_score\n",
    "\n",
    "            routed_llms = [top1_idx]\n",
    "            use_classifier = False\n",
    "            if gap < tau:\n",
    "                routed_llms.append(top2_idx)\n",
    "                use_classifier = True\n",
    "\n",
    "            routed_set_contains_best.append(true_best in routed_llms)\n",
    "\n",
    "            # Final pick:\n",
    "            if not use_classifier:\n",
    "                selected_idx = top1_idx\n",
    "            else:\n",
    "                llmA_idx, llmB_idx = top1_idx, top2_idx\n",
    "                llmA_full = llm_keys[llmA_idx]\n",
    "                llmB_full = llm_keys[llmB_idx]\n",
    "                llmA_name = key_to_short[llmA_full]\n",
    "                llmB_name = key_to_short[llmB_full]\n",
    "                pair = pd.DataFrame([[llmA_name, llmB_name]], columns=[\"llm_A\", \"llm_B\"])\n",
    "                pair_cat = encoder_cls.transform(pair)\n",
    "                mlp_input = np.hstack([emb, pair_cat[0]])\n",
    "                prob = mlp.predict_proba([mlp_input])[0][1]\n",
    "                selected_idx = llmA_idx if prob >= 0.5 else llmB_idx\n",
    "\n",
    "            overall_selection_correct.append(selected_idx == true_best)\n",
    "\n",
    "        routed_acc = np.mean(routed_set_contains_best)\n",
    "        overall_acc = np.mean(overall_selection_correct)\n",
    "        routed_accuracies.append(routed_acc)\n",
    "        selection_accuracies.append(overall_acc)\n",
    "        print(f\"  tau={tau:.2f}: Coverage={routed_acc:.3f}, Overall Selection={overall_acc:.3f}\")\n",
    "\n",
    "    # Save and plot per macro-category\n",
    "    category_results[macro_cat] = pd.DataFrame({\n",
    "        \"Threshold\": thresholds,\n",
    "        \"Coverage Accuracy\": routed_accuracies,\n",
    "        \"Overall Selection Accuracy\": selection_accuracies\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(thresholds, routed_accuracies, marker='o', label=\"Coverage Accuracy\")\n",
    "    plt.plot(thresholds, selection_accuracies, marker='s', label=\"Overall Selection Accuracy\")\n",
    "    plt.xlabel(\"Gap Threshold ($\\\\tau$)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"Macro-Category: {macro_cat} — Confidence-Based Routing\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# category_results[\"Coding\"], etc. now hold each macro-category's table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "for macro_cat, df in category_results.items():\n",
    "    plt.plot(df[\"Threshold\"], df[\"Overall Selection Accuracy\"], marker='o', label=macro_cat, linewidth=2, markersize=7)\n",
    "\n",
    "plt.axvline(x=0.12, color='gray', linestyle='--', linewidth=2)  # Vertical line at tau=0.12\n",
    "\n",
    "plt.xlabel(\"Gap Threshold ($\\\\tau$)\", fontsize=18)\n",
    "plt.ylabel(\"Overall Selection Accuracy\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(title=\"Category\", fontsize=15, title_fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"macro_category_selection_accuracy.pdf\")   # Save as PDF\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Load and prepare the data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "llm_keys = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"perplexity/sonar\"\n",
    "]\n",
    "y_scores_df = pd.DataFrame(df_processed[\"scores\"].tolist(), columns=llm_keys)\n",
    "Y = y_scores_df.values\n",
    "\n",
    "# Macro-category mapping\n",
    "category_map = {\n",
    "    'Summaries': ['summaries'],\n",
    "    'English': ['english_1', 'english_2'],\n",
    "    'Math': ['math_1', 'math_2'],\n",
    "    'Coding': ['coding_1', 'coding_2', 'coding_3'],\n",
    "    'Reasoning': ['reasoning_1', 'reasoning_2', 'reasoning_3']\n",
    "}\n",
    "\n",
    "# Map Source to macro-category\n",
    "df_processed[\"Macro\"] = None\n",
    "for macro_cat, subcats in category_map.items():\n",
    "    df_processed.loc[df_processed[\"Source\"].isin(subcats), \"Macro\"] = macro_cat\n",
    "macro_label_array = df_processed[\"Macro\"].values\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, Y_train, Y_test, macro_train, macro_test = train_test_split(\n",
    "    X_norm, Y, macro_label_array, test_size=0.20, stratify=macro_label_array, random_state=42\n",
    ")\n",
    "\n",
    "# Train regressor\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1\n",
    "))\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# Load pairwise classifier\n",
    "with open(\"df_pairwise_v2.pkl\", \"rb\") as f:\n",
    "    df_pairwise = pickle.load(f)\n",
    "X_embed_cls = np.vstack(df_pairwise[\"embedding\"].values)\n",
    "encoder_cls = OneHotEncoder(sparse_output=False)\n",
    "X_cat_cls = encoder_cls.fit_transform(df_pairwise[[\"llm_A\", \"llm_B\"]])\n",
    "X_cls = np.hstack([X_embed_cls, X_cat_cls])\n",
    "y_cls = df_pairwise[\"label\"].values\n",
    "X_train_cls, X_val_cls, y_train_cls, y_val_cls = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=42\n",
    ")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "key_to_short = {\n",
    "    \"openai/gpt-4o\": \"gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\": \"claude\",\n",
    "    \"deepseek/deepseek-chat\": \"deepseek\",\n",
    "    \"perplexity/sonar\": \"perplexity\"\n",
    "}\n",
    "\n",
    "# === Per macro-category evaluation ===\n",
    "thresholds = np.arange(0.01, 0.21, 0.01)\n",
    "category_results = {}\n",
    "\n",
    "for macro_cat, subcats in category_map.items():\n",
    "    idx = [i for i, c in enumerate(macro_test) if c == macro_cat]\n",
    "    if len(idx) == 0:\n",
    "        print(f\"[!] No test samples for macro-category {macro_cat}, skipping.\")\n",
    "        continue\n",
    "    print(f\"\\n=== Macro-category: {macro_cat} ({len(idx)} samples) ===\")\n",
    "    routed_accuracies = []\n",
    "    selection_accuracies = []\n",
    "    classifier_usages = []\n",
    "\n",
    "    for tau in thresholds:\n",
    "        routed_set_contains_best = []\n",
    "        overall_selection_correct = []\n",
    "        classifier_flags = []\n",
    "\n",
    "        for j in idx:\n",
    "            emb = X_test[j]\n",
    "            y_pred = Y_pred[j]\n",
    "            true_best = np.argmax(Y_test[j])\n",
    "            sorted_preds = np.sort(y_pred)[::-1]\n",
    "            ranked_preds = np.argsort(y_pred)[::-1]\n",
    "            top1_idx = ranked_preds[0]\n",
    "            top2_idx = ranked_preds[1]\n",
    "            top1_score = sorted_preds[0]\n",
    "            top2_score = sorted_preds[1]\n",
    "            gap = top1_score - top2_score\n",
    "\n",
    "            routed_llms = [top1_idx]\n",
    "            use_classifier = False\n",
    "            if gap < tau:\n",
    "                routed_llms.append(top2_idx)\n",
    "                use_classifier = True\n",
    "                classifier_flags.append(1)\n",
    "            else:\n",
    "                classifier_flags.append(0)\n",
    "\n",
    "            routed_set_contains_best.append(true_best in routed_llms)\n",
    "\n",
    "            # Final pick:\n",
    "            if not use_classifier:\n",
    "                selected_idx = top1_idx\n",
    "            else:\n",
    "                llmA_idx, llmB_idx = top1_idx, top2_idx\n",
    "                llmA_full = llm_keys[llmA_idx]\n",
    "                llmB_full = llm_keys[llmB_idx]\n",
    "                llmA_name = key_to_short[llmA_full]\n",
    "                llmB_name = key_to_short[llmB_full]\n",
    "                pair = pd.DataFrame([[llmA_name, llmB_name]], columns=[\"llm_A\", \"llm_B\"])\n",
    "                pair_cat = encoder_cls.transform(pair)\n",
    "                mlp_input = np.hstack([emb, pair_cat[0]])\n",
    "                prob = mlp.predict_proba([mlp_input])[0][1]\n",
    "                selected_idx = llmA_idx if prob >= 0.5 else llmB_idx\n",
    "\n",
    "            overall_selection_correct.append(selected_idx == true_best)\n",
    "\n",
    "        routed_acc = np.mean(routed_set_contains_best)\n",
    "        overall_acc = np.mean(overall_selection_correct)\n",
    "        classifier_usage = np.mean(classifier_flags)\n",
    "\n",
    "        routed_accuracies.append(routed_acc)\n",
    "        selection_accuracies.append(overall_acc)\n",
    "        classifier_usages.append(classifier_usage)\n",
    "\n",
    "        print(f\"  tau={tau:.2f}: Coverage={routed_acc:.3f}, Overall Selection={overall_acc:.3f}, Classifier Usage={classifier_usage:.3f}\")\n",
    "\n",
    "    category_results[macro_cat] = pd.DataFrame({\n",
    "        \"Threshold\": thresholds,\n",
    "        \"Coverage Accuracy\": routed_accuracies,\n",
    "        \"Overall Selection Accuracy\": selection_accuracies,\n",
    "        \"Classifier Usage\": classifier_usages\n",
    "    })\n",
    "\n",
    "    # Optional plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(thresholds, routed_accuracies, marker='o', label=\"Coverage Accuracy\")\n",
    "    plt.plot(thresholds, selection_accuracies, marker='s', label=\"Overall Selection Accuracy\")\n",
    "    plt.plot(thresholds, classifier_usages, marker='^', label=\"Classifier Usage\", linestyle='--')\n",
    "    plt.xlabel(\"Gap Threshold ($\\\\tau$)\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.title(f\"Macro-Category: {macro_cat} — Confidence-Based Routing\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# You can now access category_results[\"Reasoning\"], etc. for tables\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
