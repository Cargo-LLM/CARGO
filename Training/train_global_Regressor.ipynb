{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "import tabulate\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, \n",
    "                             confusion_matrix, accuracy_score, precision_score, recall_score, f1_score)\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "from itertools import product\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Load data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "# === Feature and Target ===\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "y_scores_df = pd.DataFrame(\n",
    "    df_processed[\"scores\"].tolist(),\n",
    "    columns=[\n",
    "        \"openai/gpt-4o\",\n",
    "        \"anthropic/claude-3.5-sonnet\",\n",
    "        \"deepseek/deepseek-chat\",\n",
    "        \"perplexity/sonar\"\n",
    "    ]\n",
    ")\n",
    "Y = y_scores_df.values\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "\n",
    "# === Train/Test Split ===\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm,\n",
    "    Y,\n",
    "    test_size=0.20,\n",
    "    stratify=category_labels,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# === Define all regression models ===\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1)),\n",
    "    \n",
    "    \"XGBoost\": MultiOutputRegressor(XGBRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1, objective=\"reg:squarederror\", random_state=42, n_jobs=-1)),\n",
    "    \n",
    "    \"MLP\": MultiOutputRegressor(MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 128), activation='relu', solver='adam', max_iter=300, random_state=42)),\n",
    "    \n",
    "    \"Ridge\": MultiOutputRegressor(Ridge(alpha=1.0))\n",
    "}\n",
    "\n",
    "# === Run training and evaluation ===\n",
    "results = {}\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # === Per-LLM MSE ===\n",
    "    per_llm_mse = [\n",
    "        mean_squared_error(Y_test[:, i], Y_pred[:, i])\n",
    "        for i in range(Y.shape[1])\n",
    "    ]\n",
    "\n",
    "    # === Top-1 Accuracy ===\n",
    "    true_best = np.argmax(Y_test, axis=1)\n",
    "    pred_best = np.argmax(Y_pred, axis=1)\n",
    "    top1_acc = accuracy_score(true_best, pred_best)\n",
    "\n",
    "    # === Top-2 Inclusion (Hit@2) ===\n",
    "    sorted_pred = np.argsort(Y_pred, axis=1)[:, ::-1]  # descending\n",
    "    top2_pred = [set(row[:2]) for row in sorted_pred]\n",
    "    hit_at_2 = np.array([\n",
    "        true_best[i] in top2_pred[i] for i in range(len(true_best))\n",
    "    ], dtype=int)\n",
    "    hit_at_2_acc = np.mean(hit_at_2)\n",
    "\n",
    "    # === Save results ===\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"Y_pred\": Y_pred,\n",
    "        \"MSE_per_LLM\": per_llm_mse,\n",
    "        \"Top1 Accuracy\": top1_acc,\n",
    "        \"Top2 Inclusion\": hit_at_2_acc\n",
    "    }\n",
    "\n",
    "    # === Print basic results ===\n",
    "    print(f\"Per-LLM MSE: {per_llm_mse}\")\n",
    "    print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "    print(f\"Top-2 Inclusion: {hit_at_2_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report True Scores (All 4 LLMs, from Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ground-truth scores across all prompts (full dataset)\n",
    "Y_full_sorted = np.sort(Y, axis=1)[:, ::-1]\n",
    "\n",
    "# Extract per-rank scores across full dataset\n",
    "rank1_all = Y_full_sorted[:, 0]\n",
    "rank2_all = Y_full_sorted[:, 1]\n",
    "rank3_all = Y_full_sorted[:, 2]\n",
    "rank4_all = Y_full_sorted[:, 3]\n",
    "\n",
    "# Print statistics\n",
    "print(\"=== Ranked Score Statistics Across Full Dataset (All Prompts) ===\")\n",
    "print(f\"Rank 1: Mean={np.mean(rank1_all):.4f}, Min={np.min(rank1_all):.4f}, Max={np.max(rank1_all):.4f}\")\n",
    "print(f\"Rank 2: Mean={np.mean(rank2_all):.4f}, Min={np.min(rank2_all):.4f}, Max={np.max(rank2_all):.4f}\")\n",
    "print(f\"Rank 3: Mean={np.mean(rank3_all):.4f}, Min={np.min(rank3_all):.4f}, Max={np.max(rank3_all):.4f}\")\n",
    "print(f\"Rank 4: Mean={np.mean(rank4_all):.4f}, Min={np.min(rank4_all):.4f}, Max={np.max(rank4_all):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set consistent styling\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "plt.figure(figsize=(5.5, 4))  # Match previous plot dimensions\n",
    "sns.boxplot(\n",
    "    data=[rank1_all, rank2_all, rank3_all, rank4_all],\n",
    "    width=0.4,\n",
    "    linewidth=1.4\n",
    ")\n",
    "\n",
    "# Axis labels and ticks\n",
    "plt.xticks(\n",
    "    range(4),\n",
    "    ['Rank 1', 'Rank 2', 'Rank 3', 'Rank 4'],\n",
    "    fontsize=12\n",
    ")\n",
    "plt.yticks([0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40], fontsize=12)\n",
    "plt.ylabel(\"Normalized Score\", fontsize=14)\n",
    "plt.xlabel(\"\", fontsize=14)\n",
    "\n",
    "# Optional horizontal gridlines\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.ylim(0.08, 0.42)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"score_distributions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Clean renaming\n",
    "y_scores_df_clean = y_scores_df.rename(columns={\n",
    "    \"openai/gpt-4o\": \"GPT-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\": \"Claude 3.5 Sonnet\",\n",
    "    \"deepseek/deepseek-chat\": \"DeepSeek Chat\",\n",
    "    \"perplexity/sonar\": \"Perplexity Sonar\"\n",
    "})\n",
    "\n",
    "# Melt for seaborn\n",
    "y_melted = y_scores_df_clean.melt(var_name=\"LLM\", value_name=\"Normalized Score\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5.5, 4))  # narrower width to pull boxes closer\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "sns.violinplot(\n",
    "    x=\"LLM\", \n",
    "    y=\"Normalized Score\", \n",
    "    data=y_melted,\n",
    "    inner=\"box\", \n",
    "    cut=0,\n",
    "    linewidth=1.4,\n",
    "    scale='area'\n",
    ")\n",
    "\n",
    "# Grid and ticks\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.xticks(rotation=10, fontsize=12)\n",
    "plt.yticks([0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40], fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Normalized Score\", fontsize=14)\n",
    "plt.ylim(0.08, 0.42)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"fig_violin_per_llm_clean.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predicted values from Random Forest\n",
    "Y_pred_rf = results[\"Random Forest\"][\"Y_pred\"]\n",
    "\n",
    "# Sorted scores descending\n",
    "sorted_preds = np.sort(Y_pred_rf, axis=1)[:, ::-1]\n",
    "top1, top2, top3 = sorted_preds[:, 0], sorted_preds[:, 1], sorted_preds[:, 2]\n",
    "\n",
    "gap_1_2 = top1 - top2\n",
    "gap_1_3 = top1 - top3\n",
    "\n",
    "# Shared bin edges for consistent x-axis\n",
    "gap_min = min(gap_1_2.min(), gap_1_3.min())\n",
    "gap_max = max(gap_1_2.max(), gap_1_3.max())\n",
    "bins = np.linspace(gap_min, gap_max, 31)  # 30 bins\n",
    "\n",
    "# Compute histogram values to match y-axis\n",
    "counts_1_2, _ = np.histogram(gap_1_2, bins=bins)\n",
    "counts_1_3, _ = np.histogram(gap_1_3, bins=bins)\n",
    "max_freq = max(counts_1_2.max(), counts_1_3.max())\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12\n",
    "})\n",
    "\n",
    "# Histogram for Rank-1 vs Rank-2\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(gap_1_2, bins=bins, edgecolor='black', color='skyblue')\n",
    "plt.xlabel(\"Score Gap\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.ylim(0, max_freq + 5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig_gap_rank12.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Histogram for Rank-1 vs Rank-3\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(gap_1_3, bins=bins, edgecolor='black', color='salmon')\n",
    "plt.xlabel(\"Score Gap\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.ylim(0, max_freq + 5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig_gap_rank13.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get true scores\n",
    "Y_true = Y  # already normalized scores, shape = (N, 4)\n",
    "\n",
    "# Compute sorted true scores (descending)\n",
    "sorted_true_scores = np.sort(Y_true, axis=1)[:, ::-1]  # shape = (N, 4)\n",
    "rank1 = sorted_true_scores[:, 0]\n",
    "rank2 = sorted_true_scores[:, 1]\n",
    "rank3 = sorted_true_scores[:, 2]\n",
    "rank4 = sorted_true_scores[:, 3]\n",
    "\n",
    "# === 1. Boxplot: Score Distributions by Rank ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot([rank1, rank2, rank3, rank4], labels=[\"Rank 1\", \"Rank 2\", \"Rank 3\", \"Rank 4\"])\n",
    "plt.title(\"Boxplot of True Scores by Rank\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 2. Histograms (Same y-axis for visual comparison) ===\n",
    "# Compute common y-limit\n",
    "counts_r1, _ = np.histogram(rank1, bins=30)\n",
    "counts_r2, _ = np.histogram(rank2, bins=30)\n",
    "counts_r3, _ = np.histogram(rank3, bins=30)\n",
    "counts_r4, _ = np.histogram(rank4, bins=30)\n",
    "max_freq_all = max(counts_r1.max(), counts_r2.max(), counts_r3.max(), counts_r4.max())\n",
    "\n",
    "# Plot histogram for each rank\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "ranks = [rank1, rank2, rank3, rank4]\n",
    "titles = [\"Rank 1\", \"Rank 2\", \"Rank 3\", \"Rank 4\"]\n",
    "colors = ['green', 'blue', 'orange', 'red']\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.hist(ranks[i], bins=30, edgecolor='black', color=colors[i])\n",
    "    ax.set_title(f\"Score Distribution: {titles[i]}\")\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_ylim(0, max_freq_all + 5)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted ranking indices from RF model\n",
    "sorted_pred_idx = np.argsort(results[\"Random Forest\"][\"Y_pred\"], axis=1)[:, ::-1]\n",
    "\n",
    "# Extract true scores of predicted top-1 to top-4\n",
    "true_scores_top1 = [Y_test[i, sorted_pred_idx[i][0]] for i in range(len(Y_test))]\n",
    "true_scores_top2 = [Y_test[i, sorted_pred_idx[i][1]] for i in range(len(Y_test))]\n",
    "true_scores_top3 = [Y_test[i, sorted_pred_idx[i][2]] for i in range(len(Y_test))]\n",
    "true_scores_top4 = [Y_test[i, sorted_pred_idx[i][3]] for i in range(len(Y_test))]\n",
    "\n",
    "# Report statistics\n",
    "print(\"=== True Scores for Top Ranked Predicted Models (Random Forest) ===\")\n",
    "print(f\"Top-1: Mean={np.mean(true_scores_top1):.4f}, Min={np.min(true_scores_top1):.4f}, Max={np.max(true_scores_top1):.4f}\")\n",
    "print(f\"Top-2: Mean={np.mean(true_scores_top2):.4f}, Min={np.min(true_scores_top2):.4f}, Max={np.max(true_scores_top2):.4f}\")\n",
    "print(f\"Top-3: Mean={np.mean(true_scores_top3):.4f}, Min={np.min(true_scores_top3):.4f}, Max={np.max(true_scores_top3):.4f}\")\n",
    "print(f\"Top-4: Mean={np.mean(true_scores_top4):.4f}, Min={np.min(true_scores_top4):.4f}, Max={np.max(true_scores_top4):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose best model result from earlier block\n",
    "best_model_name = \"Random Forest\"  # or \"Ridge\" depending on what you analyze\n",
    "Y_pred = results[best_model_name][\"Y_pred\"]\n",
    "\n",
    "# === Score Gap Analysis ===\n",
    "sorted_preds = np.sort(Y_pred, axis=1)\n",
    "top1_vals = sorted_preds[:, -1]\n",
    "top2_vals = sorted_preds[:, -2]\n",
    "gaps = top1_vals - top2_vals\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(gaps, bins=30, edgecolor=\"black\")\n",
    "plt.title(f\"{best_model_name} — Score Gap (Top1 − Top2)\")\n",
    "plt.xlabel(\"Score Gap\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(f\"\\n=== {best_model_name} Score Gap Stats ===\")\n",
    "print(f\"Mean gap     = {np.mean(gaps):.4f}\")\n",
    "print(f\"Median gap   = {np.median(gaps):.4f}\")\n",
    "print(f\"Std. dev gap = {np.std(gaps):.4f}\")\n",
    "\n",
    "# === Top-2 Ranking Consistency ===\n",
    "true_sorted = np.argsort(Y_test, axis=1)[:, ::-1]\n",
    "pred_sorted = np.argsort(Y_pred, axis=1)[:, ::-1]\n",
    "\n",
    "def top2_match(true_row, pred_row):\n",
    "    return len(set(true_row[:2]) & set(pred_row[:2])) / 2.0\n",
    "\n",
    "top2_scores = np.array([\n",
    "    top2_match(true_sorted[i], pred_sorted[i])\n",
    "    for i in range(len(Y_test))\n",
    "])\n",
    "\n",
    "mean_top2_match = np.mean(top2_scores)\n",
    "\n",
    "print(f\"\\n=== Top-2 Ranking Consistency (set overlap) ===\")\n",
    "print(f\"Mean Top-2 Match Score: {mean_top2_match:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# True best LLM for each test sample\n",
    "true_best = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Sorted predicted indices and scores\n",
    "pred_scores = results[\"Random Forest\"][\"Y_pred\"]\n",
    "sorted_preds_idx = np.argsort(pred_scores, axis=1)[:, ::-1]\n",
    "sorted_preds_val = np.sort(pred_scores, axis=1)[:, ::-1]\n",
    "top1 = sorted_preds_idx[:, 0]\n",
    "top2 = sorted_preds_idx[:, 1]\n",
    "gap = sorted_preds_val[:, 0] - sorted_preds_val[:, 1]\n",
    "\n",
    "# Thresholds to test\n",
    "thresholds = [0.01, 0.03, 0.05, 0.08, 0.10]\n",
    "print(\"=== Confidence-Aware Fallback Results (Random Forest) ===\")\n",
    "print(f\"{'Threshold':<10} {'Acc':<6} {'Fallback%':<10} {'Avg LLMs Queried':<18}\")\n",
    "\n",
    "for t in thresholds:\n",
    "    fallback_mask = gap < t  # trigger fallback\n",
    "    acc = []\n",
    "    num_llms = []\n",
    "\n",
    "    for i in range(len(true_best)):\n",
    "        if fallback_mask[i]:\n",
    "            # Try top-1 and top-2 → correct if true_best in either\n",
    "            if true_best[i] in [top1[i], top2[i]]:\n",
    "                acc.append(1)\n",
    "            else:\n",
    "                acc.append(0)\n",
    "            num_llms.append(2)\n",
    "        else:\n",
    "            # Route only to top-1\n",
    "            acc.append(int(true_best[i] == top1[i]))\n",
    "            num_llms.append(1)\n",
    "\n",
    "    acc_val = np.mean(acc)\n",
    "    fallback_rate = np.mean(fallback_mask)\n",
    "    avg_llms = np.mean(num_llms)\n",
    "\n",
    "    print(f\"{t:<10} {acc_val:.4f}   {fallback_rate:.2f}       {avg_llms:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute if needed\n",
    "Y_pred_rf = results[\"Random Forest\"][\"Y_pred\"]\n",
    "sorted_pred_vals = np.sort(Y_pred_rf, axis=1)\n",
    "top1_vals = sorted_pred_vals[:, -1]\n",
    "top2_vals = sorted_pred_vals[:, -2]\n",
    "gaps = top1_vals - top2_vals\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(gaps, bins=30, edgecolor=\"black\")\n",
    "plt.title(\"Random Forest – Score Gap (Top-1 − Top-2)\")\n",
    "plt.xlabel(\"Score Gap\")\n",
    "plt.ylabel(\"Number of Prompts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Regression Summary Table ===\")\n",
    "print(f\"{'Model':<15} {'Top-1 Acc':<10} {'Top-2 Inc':<10} {'Avg MSE':<10}\")\n",
    "for name, data in results.items():\n",
    "    mse_avg = np.mean(data[\"MSE_per_LLM\"])\n",
    "    print(f\"{name:<15} {data['Top1 Accuracy']:<10.4f} {data['Top2 Inclusion']:<10.4f} {mse_avg:<10.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = list(results.keys())\n",
    "top1_accuracies = [results[m]['Top1 Accuracy'] for m in model_names]\n",
    "top2_inclusions = [results[m]['Top2 Inclusion'] for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width/2, top1_accuracies, width, label='Top-1 Accuracy')\n",
    "plt.bar(x + width/2, top2_inclusions, width, label='Top-2 Inclusion')\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Top-1 and Top-2 Routing Accuracy by Regression Model\")\n",
    "plt.xticks(x, model_names, rotation=15)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# === Define all regression models ===\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1)),\n",
    "    \n",
    "    \"XGBoost\": MultiOutputRegressor(XGBRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1, objective=\"reg:squarederror\", random_state=42, n_jobs=-1)),\n",
    "    \n",
    "    \"MLP\": MultiOutputRegressor(MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 128), activation='relu', solver='adam', max_iter=300, random_state=42)),\n",
    "    \n",
    "    \"Ridge\": MultiOutputRegressor(Ridge(alpha=1.0)),\n",
    "\n",
    "    \"SVR\": MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "}\n",
    "\n",
    "# === Run training and evaluation ===\n",
    "results = {}\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # === Per-LLM MSE ===\n",
    "    per_llm_mse = [\n",
    "        mean_squared_error(Y_test[:, i], Y_pred[:, i])\n",
    "        for i in range(Y.shape[1])\n",
    "    ]\n",
    "\n",
    "    # === Top-1 Accuracy ===\n",
    "    true_best = np.argmax(Y_test, axis=1)\n",
    "    pred_best = np.argmax(Y_pred, axis=1)\n",
    "    top1_acc = accuracy_score(true_best, pred_best)\n",
    "\n",
    "    # === Top-2 Inclusion (Hit@2) ===\n",
    "    sorted_pred = np.argsort(Y_pred, axis=1)[:, ::-1]  # descending\n",
    "    top2_pred = [set(row[:2]) for row in sorted_pred]\n",
    "    hit_at_2 = np.array([\n",
    "        true_best[i] in top2_pred[i] for i in range(len(true_best))\n",
    "    ], dtype=int)\n",
    "    hit_at_2_acc = np.mean(hit_at_2)\n",
    "\n",
    "    # === Save results ===\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"Y_pred\": Y_pred,\n",
    "        \"MSE_per_LLM\": per_llm_mse,\n",
    "        \"Top1 Accuracy\": top1_acc,\n",
    "        \"Top2 Inclusion\": hit_at_2_acc\n",
    "    }\n",
    "\n",
    "    # === Print basic results ===\n",
    "    print(f\"Per-LLM MSE: {per_llm_mse}\")\n",
    "    print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "    print(f\"Top-2 Inclusion: {hit_at_2_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Imports\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Define the five lightweight regressors\n",
    "# ------------------------------------------------------------\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(\n",
    "        RandomForestRegressor(\n",
    "            n_estimators=200, max_depth=25, min_samples_leaf=5,\n",
    "            random_state=42, n_jobs=-1)\n",
    "    ),\n",
    "    \"XGBoost\": MultiOutputRegressor(\n",
    "        XGBRegressor(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "            objective=\"reg:squarederror\", random_state=42, n_jobs=-1)\n",
    "    ),\n",
    "    \"MLP\": MultiOutputRegressor(\n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=(256, 128), activation='relu',\n",
    "            solver='adam', max_iter=300, random_state=42)\n",
    "    ),\n",
    "    \"Ridge\": MultiOutputRegressor(Ridge(alpha=1.0)),\n",
    "    \"SVR\": MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Train, predict, and store results\n",
    "# ------------------------------------------------------------\n",
    "results = {}\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    results[name] = {\"model\": model, \"Y_pred\": Y_pred}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Correct win-rate computation\n",
    "#    Win rate = (0.5 * N_spec + N_better) / total\n",
    "#    where\n",
    "#      N_spec   = router actually chose the reference LLM\n",
    "#      N_better = router chose a *different* LLM whose score > reference LLM\n",
    "# ------------------------------------------------------------\n",
    "def win_rate_against_llms(Y_test, Y_pred):\n",
    "    total = Y_test.shape[0]\n",
    "    pick = np.argmax(Y_pred, axis=1)                       # index router picked\n",
    "    chosen_scores = Y_test[np.arange(total), pick]         # score of router pick\n",
    "    win_rates = []\n",
    "    for llm_idx in range(Y_test.shape[1]):\n",
    "        spec_mask   = (pick == llm_idx)                    # router chose that LLM\n",
    "        n_spec      = np.sum(spec_mask)\n",
    "        llm_scores  = Y_test[:, llm_idx]                   # ground-truth scores of that LLM\n",
    "        better_mask = ~spec_mask & (chosen_scores > llm_scores)\n",
    "        n_better    = np.sum(better_mask)\n",
    "        win_rate = (0.5 * n_spec + n_better) / total\n",
    "        win_rates.append(win_rate)\n",
    "    return win_rates\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Print win-rate table\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== Win Rates per LLM (correct formula) ===\")\n",
    "headers = [\"Model\", \"GPT-4o\", \"Claude\", \"DeepSeek\", \"Perplexity\"]\n",
    "print(f\"{headers[0]:<18} {headers[1]:>10} {headers[2]:>10} {headers[3]:>10} {headers[4]:>12}\")\n",
    "\n",
    "for name, info in results.items():\n",
    "    wr = win_rate_against_llms(Y_test, info[\"Y_pred\"])\n",
    "    wr_pct = [f\"{100*r:.2f}%\" for r in wr]\n",
    "    print(f\"{name:<18} {wr_pct[0]:>10} {wr_pct[1]:>10} {wr_pct[2]:>10} {wr_pct[3]:>12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 0.  Imports\n",
    "# =======================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1.  Train / test split  (skip if you already have X_train...)\n",
    "# -------------------------------------------------------\n",
    "#   X : (N, d)  prompt embeddings\n",
    "#   Y : (N, 4)  PairRanker scores for [gpt-4o, claude-3.5, deepseek, perplexity]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=np.argmax(Y, axis=1)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2.  Define lightweight regressors\n",
    "# -------------------------------------------------------\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(\n",
    "        RandomForestRegressor(\n",
    "            n_estimators=200, max_depth=25, min_samples_leaf=5,\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "    ),\n",
    "    \"Ridge\": MultiOutputRegressor(Ridge(alpha=1.0)),\n",
    "    \"XGBoost\": MultiOutputRegressor(\n",
    "        XGBRegressor(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "            objective=\"reg:squarederror\", random_state=42, n_jobs=-1\n",
    "        )\n",
    "    ),\n",
    "    \"SVR\": MultiOutputRegressor(SVR(kernel=\"rbf\", C=1.0, epsilon=0.1)),\n",
    "    \"MLP\": MultiOutputRegressor(\n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=(256, 128), activation=\"relu\",\n",
    "            solver=\"adam\", max_iter=300, random_state=42\n",
    "        )\n",
    "    ),\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3.  Train, predict, evaluate\n",
    "# -------------------------------------------------------\n",
    "results = {}\n",
    "true_best = np.argmax(Y_test, axis=1)\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # per-LLM MSE\n",
    "    per_llm_mse = [\n",
    "        mean_squared_error(Y_test[:, i], Y_pred[:, i])\n",
    "        for i in range(Y.shape[1])\n",
    "    ]\n",
    "\n",
    "    # Top-1 accuracy\n",
    "    pred_best = np.argmax(Y_pred, axis=1)\n",
    "    top1_acc = accuracy_score(true_best, pred_best)\n",
    "\n",
    "    # Hit@2\n",
    "    top2_pred = np.argsort(Y_pred, axis=1)[:, ::-1][:, :2]\n",
    "    hit2 = [true_best[i] in top2_pred[i] for i in range(len(true_best))]\n",
    "    hit2_acc = np.mean(hit2)\n",
    "\n",
    "    # store\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"Y_pred\": Y_pred,\n",
    "        \"MSE_per_LLM\": per_llm_mse,\n",
    "        \"Top1 Accuracy\": top1_acc,\n",
    "        \"Hit@2\": hit2_acc,\n",
    "    }\n",
    "\n",
    "    # quick printout\n",
    "    print(f\"Per-LLM MSE : {[f'{m:.4f}' for m in per_llm_mse]}\")\n",
    "    print(f\"Top-1 Acc   : {top1_acc:.4f}\")\n",
    "    print(f\"Hit@2       : {hit2_acc:.4f}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4.  Grouped bar chart: Top-1 vs Hit@2\n",
    "# -------------------------------------------------------\n",
    "regressor_names = list(results.keys())\n",
    "top1_vals = [results[r][\"Top1 Accuracy\"] for r in regressor_names]\n",
    "hit2_vals = [results[r][\"Hit@2\"] for r in regressor_names]\n",
    "\n",
    "# sort by descending Top-1 for nicer layout\n",
    "order = np.argsort(top1_vals)[::-1]\n",
    "regressor_names = [regressor_names[i] for i in order]\n",
    "top1_vals = [top1_vals[i] for i in order]\n",
    "hit2_vals = [hit2_vals[i] for i in order]\n",
    "\n",
    "x = np.arange(len(regressor_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(9, 4.5))\n",
    "bars1 = plt.bar(x - width / 2, top1_vals, width, label=\"Top-1\")\n",
    "bars2 = plt.bar(x + width / 2, hit2_vals, width, label=\"Hit@2\")\n",
    "\n",
    "plt.ylabel(\"Fraction of prompts\")\n",
    "plt.title(\"Overall Routing Performance\")\n",
    "plt.xticks(x, regressor_names, rotation=15)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "\n",
    "# annotate bars\n",
    "for bar in list(bars1) + list(bars2):\n",
    "    h = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, h + 0.02,\n",
    "             f\"{h:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Load and prepare data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "# Features and labels\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "y_scores_df = pd.DataFrame(\n",
    "    df_processed[\"scores\"].tolist(),\n",
    "    columns=[\n",
    "        \"openai/gpt-4o\",\n",
    "        \"anthropic/claude-3.5-sonnet\",\n",
    "        \"deepseek/deepseek-chat\",\n",
    "        \"perplexity/sonar\"\n",
    "    ]\n",
    ")\n",
    "Y = y_scores_df.values\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Train-test split (stratified by Source)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm,\n",
    "    Y,\n",
    "    test_size=0.20,\n",
    "    stratify=category_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === Train the Random Forest regression model ===\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1\n",
    "))\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# === Confidence-Aware Fallback Analysis ===\n",
    "true_top1 = np.argmax(Y_test, axis=1)\n",
    "sorted_preds = np.sort(Y_pred, axis=1)[:, ::-1]\n",
    "ranked_preds = np.argsort(Y_pred, axis=1)[:, ::-1]\n",
    "\n",
    "top1_idx = ranked_preds[:, 0]\n",
    "top2_idx = ranked_preds[:, 1]\n",
    "top1_score = sorted_preds[:, 0]\n",
    "top2_score = sorted_preds[:, 1]\n",
    "\n",
    "# Compute confidence gap\n",
    "gap = top1_score - top2_score\n",
    "\n",
    "# Thresholds to test\n",
    "thresholds = np.arange(0.01, 0.21, 0.01)\n",
    "fallback_stats = []\n",
    "\n",
    "for tau in thresholds:\n",
    "    fallback_mask = gap < tau\n",
    "    routed_llms = np.where(\n",
    "        fallback_mask[:, None],\n",
    "        np.stack([top1_idx, top2_idx], axis=1),\n",
    "        np.stack([top1_idx, top1_idx], axis=1)  # duplicate if not falling back\n",
    "    )\n",
    "\n",
    "    # Accuracy check: top-1 in ground truth within routed\n",
    "    correct = [(true_top1[i] in routed_llms[i]) for i in range(len(true_top1))]\n",
    "    accuracy = np.mean(correct)\n",
    "    fallback_pct = np.mean(fallback_mask)\n",
    "    avg_llms_queried = 1 + fallback_pct\n",
    "\n",
    "    fallback_stats.append((tau, accuracy, fallback_pct, avg_llms_queried))\n",
    "\n",
    "# Convert to DataFrame\n",
    "fallback_df = pd.DataFrame(\n",
    "    fallback_stats,\n",
    "    columns=[\"Threshold\", \"Accuracy\", \"Fallback Rate\", \"Avg. LLMs Queried\"]\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Confidence-Aware Fallback Results (Random Forest) ===\")\n",
    "print(fallback_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fallback_df[\"Avg. LLMs Queried\"],\n",
    "         fallback_df[\"Accuracy\"]*100,\n",
    "         marker=\"o\")\n",
    "for _, row in fallback_df.iterrows():\n",
    "    if row[\"Threshold\"] in {0.03,0.05,0.10}:\n",
    "        plt.annotate(f\"τ={row['Threshold']:.2f}\",\n",
    "                     (row[\"Avg. LLMs Queried\"], row[\"Accuracy\"]*100),\n",
    "                     textcoords=\"offset points\", xytext=(0,6), ha='center')\n",
    "plt.xlabel(\"Average LLMs Queried\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acc_vs_cost_rf.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Load enhanced pairwise dataset ===\n",
    "with open(\"df_pairwise_v2.pkl\", \"rb\") as f:\n",
    "    df_pairwise = pickle.load(f)\n",
    "\n",
    "# === Extract prompt embeddings (numerical)\n",
    "X_embed = np.vstack(df_pairwise[\"embedding\"].values)\n",
    "\n",
    "# === One-hot encode llm_A, llm_B, and Source (categorical)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_cat = encoder.fit_transform(df_pairwise[[\"llm_A\", \"llm_B\", \"Source\"]])\n",
    "\n",
    "# === Final input: [embedding + one-hot categorical features]\n",
    "X = np.hstack([X_embed, X_cat])\n",
    "\n",
    "# === Labels\n",
    "y = df_pairwise[\"label\"].values\n",
    "\n",
    "# === Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=25, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n✅ Realistic Random Forest Accuracy (no leakage): {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# === Load dataset ===\n",
    "with open(\"df_pairwise_v2.pkl\", \"rb\") as f:\n",
    "    df_pairwise = pickle.load(f)\n",
    "\n",
    "# === Extract prompt embeddings\n",
    "X_embed = np.vstack(df_pairwise[\"embedding\"].values)\n",
    "\n",
    "# === One-hot encode llm_A and llm_B\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_cat = encoder.fit_transform(df_pairwise[[\"llm_A\", \"llm_B\"]])\n",
    "\n",
    "# === Final feature matrix\n",
    "X = np.hstack([X_embed, X_cat])\n",
    "y = df_pairwise[\"label\"].values\n",
    "\n",
    "# === Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Define classifiers\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=25, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=300, random_state=42),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"SVC\": SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# === Evaluate each model\n",
    "print(f\"{'Model':20s}  Acc   Prec  Recall   F1     AUC\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # fallback for models without predict_proba\n",
    "        y_prob = y_pred\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    print(f\"{name:20s}  {acc:.3f}  {prec:.3f}  {rec:.3f}   {f1:.3f}  {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, itertools, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0 · Assumed pre-computed objects\n",
    "#     X_train, Y_train, X_test, Y_test from your earlier split\n",
    "#     llm_cols list (length 4) with fixed order\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 · (ROUTER) Train multi-output regressors  -----------------------\n",
    "# ------------------------------------------------------------------\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(\n",
    "        RandomForestRegressor(n_estimators=200, max_depth=25,\n",
    "                              min_samples_leaf=5, random_state=42, n_jobs=-1))\n",
    "}\n",
    "\n",
    "router_results = {}\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n=== Training router: {name} ===\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    true_best = np.argmax(Y_test, axis=1)\n",
    "    pred_best = np.argmax(Y_pred, axis=1)\n",
    "    top1_acc  = accuracy_score(true_best, pred_best)\n",
    "\n",
    "    # Hit@2 (Top-2 inclusion)\n",
    "    sorted_pred = np.argsort(Y_pred, axis=1)[:, ::-1]\n",
    "    hit2 = [true_best[i] in sorted_pred[i, :2] for i in range(len(true_best))]\n",
    "    hit2_acc = np.mean(hit2)\n",
    "\n",
    "    router_results[name] = dict(model=model,\n",
    "                                Y_pred=Y_pred,\n",
    "                                Top1=top1_acc,\n",
    "                                Hit2=hit2_acc)\n",
    "    print(f\"Top-1: {top1_acc:.4f}  ·  Hit@2: {hit2_acc:.4f}\")\n",
    "\n",
    "# Choose the Random Forest router for the rest\n",
    "rf_router = router_results[\"Random Forest\"][\"model\"]\n",
    "Y_pred_rf = router_results[\"Random Forest\"][\"Y_pred\"]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2 · (SELECTOR) Load pair-wise dataset and train classifier --------\n",
    "# ------------------------------------------------------------------\n",
    "pair_df = pd.read_pickle(\"pairwise_llm_dataset_v2.pkl\")\n",
    "\n",
    "# Extract features\n",
    "emb_mat   = np.vstack(pair_df[\"embedding\"].values)          # (M, 1536)\n",
    "llm_a_id  = pair_df[\"LLM_A_id\"].values.reshape(-1, 1)\n",
    "llm_b_id  = pair_df[\"LLM_B_id\"].values.reshape(-1, 1)\n",
    "\n",
    "# One-hot encode A and B IDs (4 classes each)\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "llm_onehot = enc.fit_transform(np.hstack([llm_a_id, llm_b_id]))  # (M, 8)\n",
    "\n",
    "X_pair = np.hstack([emb_mat, llm_onehot])   # final feat dim = 1536 + 8\n",
    "y_pair = pair_df[\"label\"].values\n",
    "\n",
    "# Split\n",
    "Xp_tr, Xp_te, yp_tr, yp_te = train_test_split(\n",
    "        X_pair, y_pair, test_size=0.2, random_state=42, stratify=y_pair)\n",
    "\n",
    "# Train lightweight selector (LogReg)\n",
    "selector = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "selector.fit(Xp_tr, yp_tr)\n",
    "\n",
    "print(\"\\n=== Selector validation metrics ===\")\n",
    "print(classification_report(yp_te, selector.predict(Xp_te), digits=4))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3 · Pipeline evaluation (router + selector) ----------------------\n",
    "# ------------------------------------------------------------------\n",
    "def pipeline_decision(emb, pred_scores, tau=0.05):\n",
    "    order = np.argsort(pred_scores)[::-1]\n",
    "    top1, top2 = order[:2]\n",
    "    gap = pred_scores[top1] - pred_scores[top2]\n",
    "\n",
    "    if gap >= tau:\n",
    "        return top1                       # single query case\n",
    "    # build selector feature\n",
    "    pair_feat = np.hstack([\n",
    "        emb,\n",
    "        enc.transform([[top1, top2]])[0]  # one-hot A,B\n",
    "    ])\n",
    "    return top1 if selector.predict(pair_feat.reshape(1, -1))[0] == 1 else top2\n",
    "\n",
    "tau = 0.05\n",
    "final_preds = []\n",
    "for i in range(len(X_test)):\n",
    "    best_idx = pipeline_decision(X_test[i], Y_pred_rf[i], tau)\n",
    "    final_preds.append(best_idx)\n",
    "\n",
    "final_acc = accuracy_score(np.argmax(Y_test, axis=1), final_preds)\n",
    "\n",
    "print(\"\\n=== Pipeline results (τ = 0.05) ===\")\n",
    "print(f\"Router Top-1: {router_results['Random Forest']['Top1']:.4f}\")\n",
    "print(f\"Router Hit@2: {router_results['Random Forest']['Hit2']:.4f}\")\n",
    "print(f\"Pipeline Top-1 (after selector): {final_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 0 · Imports\n",
    "# ================================================================\n",
    "import pickle, itertools, joblib\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# ================================================================\n",
    "# 1 · Load df_processed and build primary matrices\n",
    "# ================================================================\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "# prompt embeddings\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, axis=1)                         # (N, 1536)\n",
    "\n",
    "# true score matrix\n",
    "llm_cols = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"perplexity/sonar\"\n",
    "]\n",
    "Y = pd.DataFrame(df_processed[\"scores\"].tolist(),\n",
    "                 columns=llm_cols).values             # (N, 4)\n",
    "\n",
    "# ================================================================\n",
    "# 2 · Train/test split for router (stratify if 'Source' exists)\n",
    "# ================================================================\n",
    "if \"Source\" in df_processed.columns:\n",
    "    stratify_arg = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "else:\n",
    "    print(\"Warning: 'Source' column not found — using unstratified split.\")\n",
    "    stratify_arg = None\n",
    "\n",
    "X_tr_r, X_te_r, Y_tr_r, Y_te_r = train_test_split(\n",
    "    X_norm, Y, test_size=0.20, random_state=42, stratify=stratify_arg)\n",
    "\n",
    "# ================================================================\n",
    "# 3 · ROUTER — Random-Forest multi-output regressor\n",
    "# ================================================================\n",
    "rf_router = MultiOutputRegressor(\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=25,\n",
    "        min_samples_leaf=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42))\n",
    "rf_router.fit(X_tr_r, Y_tr_r)\n",
    "Y_pred_r = rf_router.predict(X_te_r)\n",
    "\n",
    "# Router metrics\n",
    "true_best = np.argmax(Y_te_r, axis=1)\n",
    "pred_best = np.argmax(Y_pred_r, axis=1)\n",
    "router_top1 = accuracy_score(true_best, pred_best)\n",
    "hit2 = [\n",
    "    true_best[i] in np.argsort(Y_pred_r[i])[::-1][:2]\n",
    "    for i in range(len(true_best))\n",
    "]\n",
    "router_hit2 = np.mean(hit2)\n",
    "\n",
    "print(\"\\n=== ROUTER RESULTS ===\")\n",
    "print(f\"Top-1 accuracy : {router_top1:.4f}\")\n",
    "print(f\"Hit@2          : {router_hit2:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# 4 · Build pair-wise dataset (skip ties) for selector\n",
    "# ================================================================\n",
    "pair_rows = []\n",
    "for emb_i, scores_i in zip(X_norm, Y):\n",
    "    for a, b in itertools.combinations(range(4), 2):\n",
    "        if np.isclose(scores_i[a], scores_i[b]):       # skip exact ties\n",
    "            continue\n",
    "        winner, loser = (a, b) if scores_i[a] > scores_i[b] else (b, a)\n",
    "\n",
    "        # winner row  (label 1)\n",
    "        pair_rows.append((emb_i, winner, loser, 1))\n",
    "        # loser row   (label 0)\n",
    "        pair_rows.append((emb_i, loser, winner, 0))\n",
    "\n",
    "pair_df = pd.DataFrame(pair_rows,\n",
    "                       columns=[\"embedding\", \"LLM_A\", \"LLM_B\", \"label\"])\n",
    "\n",
    "# ================================================================\n",
    "# 5 · FEATURES for selector  (embedding + one-hot IDs)\n",
    "# ================================================================\n",
    "emb_mat = np.vstack(pair_df[\"embedding\"].values)\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "llm_onehot = enc.fit_transform(pair_df[[\"LLM_A\", \"LLM_B\"]].values)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "emb_scaled = scaler.fit_transform(emb_mat)\n",
    "\n",
    "X_pair = np.hstack([emb_scaled, llm_onehot]).astype(np.float32)\n",
    "y_pair = pair_df[\"label\"].values\n",
    "\n",
    "Xp_tr, Xp_te, yp_tr, yp_te = train_test_split(\n",
    "    X_pair, y_pair, test_size=0.20, random_state=42, stratify=y_pair)\n",
    "\n",
    "# ================================================================\n",
    "# 6 · SELECTOR — Random-Forest classifier\n",
    "# ================================================================\n",
    "rf_selector = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42)\n",
    "rf_selector.fit(Xp_tr, yp_tr)\n",
    "\n",
    "print(\"\\n=== SELECTOR VALIDATION METRICS ===\")\n",
    "print(classification_report(yp_te, rf_selector.predict(Xp_te), digits=4))\n",
    "\n",
    "# ================================================================\n",
    "# 7 · PIPELINE EVALUATION  (τ = 0.08)\n",
    "# ================================================================\n",
    "tau = 0.08\n",
    "final_preds = []\n",
    "\n",
    "for emb, scores_pred, true_idx in zip(X_te_r, Y_pred_r, true_best):\n",
    "    order = np.argsort(scores_pred)[::-1]\n",
    "    top1, top2 = order[:2]\n",
    "    gap = scores_pred[top1] - scores_pred[top2]\n",
    "\n",
    "    if gap >= tau:\n",
    "        final_preds.append(top1)                   # single query\n",
    "    else:\n",
    "        # build selector input: scaled embedding + one-hot pair\n",
    "        feat = np.hstack([\n",
    "            scaler.transform(emb.reshape(1, -1)),\n",
    "            enc.transform([[top1, top2]])\n",
    "        ])\n",
    "        choose_A = rf_selector.predict(feat)[0]    # 1 ⇒ A wins\n",
    "        final_preds.append(top1 if choose_A else top2)\n",
    "\n",
    "pipeline_top1 = accuracy_score(true_best, final_preds)\n",
    "\n",
    "print(\"\\n=== PIPELINE RESULTS  (τ = 0.08) ===\")\n",
    "print(f\"Router Top-1         : {router_top1:.4f}\")\n",
    "print(f\"Router Hit@2         : {router_hit2:.4f}\")\n",
    "print(f\"Pipeline Top-1       : {pipeline_top1:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# 8 · Save models and encoders (optional)\n",
    "# ================================================================\n",
    "joblib.dump(rf_router,   \"rf_router.pkl\")\n",
    "joblib.dump(rf_selector, \"rf_selector.pkl\")\n",
    "joblib.dump(enc,         \"pair_onehot_encoder.pkl\")\n",
    "joblib.dump(scaler,      \"embedding_scaler.pkl\")\n",
    "print(\"\\nSaved: rf_router.pkl, rf_selector.pkl, pair_onehot_encoder.pkl, embedding_scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 0 · Imports\n",
    "# ================================================================\n",
    "import pickle, itertools, joblib\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# ================================================================\n",
    "# 1 · ROUTER DATA  (load embeddings + scores; stratify by Source)\n",
    "# ================================================================\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "# embeddings\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "# score matrix (4 LLMs)\n",
    "llm_cols = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"perplexity/sonar\"\n",
    "]\n",
    "Y = pd.DataFrame(df_processed[\"scores\"].tolist(), columns=llm_cols).values\n",
    "\n",
    "# stratified split by Source (if available)\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "X_tr_r, X_te_r, Y_tr_r, Y_te_r = train_test_split(\n",
    "    X_norm, Y, test_size=0.20, stratify=category_labels, random_state=42)\n",
    "\n",
    "# ================================================================\n",
    "# 2 · ROUTER — Random-Forest multi-output regressor\n",
    "# ================================================================\n",
    "rf_router = MultiOutputRegressor(\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=25,\n",
    "        min_samples_leaf=5, n_jobs=-1, random_state=42)\n",
    ")\n",
    "rf_router.fit(X_tr_r, Y_tr_r)\n",
    "Y_pred_r = rf_router.predict(X_te_r)\n",
    "\n",
    "# Router metrics\n",
    "true_best = np.argmax(Y_te_r, axis=1)\n",
    "pred_best = np.argmax(Y_pred_r, axis=1)\n",
    "router_top1 = accuracy_score(true_best, pred_best)\n",
    "hit2 = [true_best[i] in np.argsort(Y_pred_r[i])[::-1][:2]\n",
    "        for i in range(len(true_best))]\n",
    "router_hit2 = np.mean(hit2)\n",
    "\n",
    "print(\"\\n=== ROUTER RESULTS ===\")\n",
    "print(f\"Top-1 accuracy : {router_top1:.4f}\")\n",
    "print(f\"Hit@2          : {router_hit2:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# 3 · SELECTOR DATA  (load pair-wise dataset)\n",
    "# ================================================================\n",
    "pair_df = pd.read_pickle(\"pairwise_llm_dataset_v2.pkl\")\n",
    "\n",
    "# build features: embedding + one-hot(LLM_A, LLM_B)\n",
    "emb_mat = np.vstack(pair_df[\"embedding\"].values)\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "llm_pair_1hot = enc.fit_transform(pair_df[[\"LLM_A_id\", \"LLM_B_id\"]].values)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "emb_scaled = scaler.fit_transform(emb_mat)\n",
    "\n",
    "X_pair = np.hstack([emb_scaled, llm_pair_1hot]).astype(np.float32)\n",
    "y_pair = pair_df[\"label\"].values\n",
    "\n",
    "Xp_tr, Xp_te, yp_tr, yp_te = train_test_split(\n",
    "    X_pair, y_pair, test_size=0.20, random_state=42, stratify=y_pair)\n",
    "\n",
    "# ================================================================\n",
    "# 4 · SELECTOR — Random-Forest binary classifier\n",
    "# ================================================================\n",
    "rf_selector = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=None,\n",
    "    min_samples_leaf=2, class_weight=\"balanced\",\n",
    "    n_jobs=-1, random_state=42)\n",
    "rf_selector.fit(Xp_tr, yp_tr)\n",
    "\n",
    "print(\"\\n=== SELECTOR VALIDATION METRICS ===\")\n",
    "print(classification_report(yp_te, rf_selector.predict(Xp_te), digits=4))\n",
    "\n",
    "# ================================================================\n",
    "# 5 · PIPELINE EVALUATION (τ = 0.08)\n",
    "# ================================================================\n",
    "tau = 0.08\n",
    "final_preds = []\n",
    "\n",
    "for emb, scores_pred in zip(X_te_r, Y_pred_r):\n",
    "    order = np.argsort(scores_pred)[::-1]\n",
    "    top1, top2 = order[:2]\n",
    "    gap = scores_pred[top1] - scores_pred[top2]\n",
    "\n",
    "    if gap >= tau:\n",
    "        final_preds.append(top1)                    # single query case\n",
    "    else:\n",
    "        # selector feature: scaled embedding ++ one-hot pair\n",
    "        feat = np.hstack([\n",
    "            scaler.transform(emb.reshape(1, -1)),\n",
    "            enc.transform([[top1, top2]])\n",
    "        ])\n",
    "        choose_A = rf_selector.predict(feat)[0]     # 1 ⇒ A wins\n",
    "        final_preds.append(top1 if choose_A else top2)\n",
    "\n",
    "pipeline_top1 = accuracy_score(true_best, final_preds)\n",
    "\n",
    "print(\"\\n=== PIPELINE RESULTS (τ = 0.08) ===\")\n",
    "print(f\"Router Top-1        : {router_top1:.4f}\")\n",
    "print(f\"Router Hit@2        : {router_hit2:.4f}\")\n",
    "print(f\"Pipeline Top-1      : {pipeline_top1:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# 6 · (Optional) save models and encoders\n",
    "# ================================================================\n",
    "joblib.dump(rf_router,   \"rf_router.pkl\")\n",
    "joblib.dump(rf_selector, \"rf_selector.pkl\")\n",
    "joblib.dump(enc,         \"pair_onehot_encoder.pkl\")\n",
    "joblib.dump(scaler,      \"embedding_scaler.pkl\")\n",
    "print(\"\\nSaved models and encoders to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# === Load and prepare your data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "y_scores_df = pd.DataFrame(\n",
    "    df_processed[\"scores\"].tolist(),\n",
    "    columns=[\n",
    "        \"openai/gpt-4o\",\n",
    "        \"anthropic/claude-3.5-sonnet\",\n",
    "        \"deepseek/deepseek-chat\",\n",
    "        \"perplexity/sonar\"\n",
    "    ]\n",
    ")\n",
    "Y = y_scores_df.values\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm,\n",
    "    Y,\n",
    "    test_size=0.20,\n",
    "    stratify=category_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === Define regression models ===\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1)),\n",
    "    \"XGBoost\": MultiOutputRegressor(XGBRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "        objective=\"reg:squarederror\", random_state=42, n_jobs=-1)),\n",
    "    \"MLP\": MultiOutputRegressor(MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 128), activation='relu',\n",
    "        solver='adam', max_iter=300, random_state=42)),\n",
    "    \"Ridge\": MultiOutputRegressor(Ridge(alpha=1.0)),\n",
    "    \"SVR\": MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# === Train, predict, and evaluate ===\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # 1) Per-LLM MSE\n",
    "    per_llm_mse = [\n",
    "        mean_squared_error(Y_test[:, i], Y_pred[:, i])\n",
    "        for i in range(Y.shape[1])\n",
    "    ]\n",
    "\n",
    "    # 2) Top-1 Accuracy\n",
    "    true_best = np.argmax(Y_test, axis=1)\n",
    "    pred_best = np.argmax(Y_pred, axis=1)\n",
    "    top1_acc = accuracy_score(true_best, pred_best)\n",
    "\n",
    "    # 3) Hit-@-2 Accuracy\n",
    "    #    Check if the true best index is in the model's top two predictions\n",
    "    sorted_idxs = np.argsort(Y_pred, axis=1)[:, ::-1]  # descending order\n",
    "    top2_sets = [set(row[:2]) for row in sorted_idxs]\n",
    "    hit2_acc = np.mean([true_best[i] in top2_sets[i] for i in range(len(true_best))])\n",
    "\n",
    "    # === Save results ===\n",
    "    results[name] = {\n",
    "        \"MSE_per_LLM\": per_llm_mse,\n",
    "        \"Top1 Accuracy\": top1_acc,\n",
    "        \"Hit@2 Accuracy\": hit2_acc\n",
    "    }\n",
    "\n",
    "    # === Print results ===\n",
    "    print(f\"Per-LLM MSE: {per_llm_mse}\")\n",
    "    print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "    print(f\"Hit@2 Accuracy: {hit2_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# === Load and prepare data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "y_scores_df = pd.DataFrame(\n",
    "    df_processed[\"scores\"].tolist(),\n",
    "    columns=[\n",
    "        \"openai/gpt-4o\",\n",
    "        \"anthropic/claude-3.5-sonnet\",\n",
    "        \"deepseek/deepseek-chat\",\n",
    "        \"perplexity/sonar\"\n",
    "    ]\n",
    ")\n",
    "Y = y_scores_df.values\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm,\n",
    "    Y,\n",
    "    test_size=0.20,\n",
    "    stratify=category_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === Define regression models ===\n",
    "regression_models = {\n",
    "    \"Random Forest\": MultiOutputRegressor(RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=25, min_samples_leaf=5,\n",
    "        random_state=42, n_jobs=-1)),\n",
    "    \"XGBoost\": MultiOutputRegressor(XGBRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "        objective=\"reg:squarederror\", random_state=42, n_jobs=-1)),\n",
    "    \"MLP\": MultiOutputRegressor(MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 128), activation='relu',\n",
    "        solver='adam', max_iter=300, random_state=42)),\n",
    "    \"Ridge\": MultiOutputRegressor(Ridge(alpha=1.0)),\n",
    "    \"SVR\": MultiOutputRegressor(SVR(kernel='rbf', C=1.0, epsilon=0.1))\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# === Train, predict, and evaluate ===\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Per-LLM MSE\n",
    "    per_llm_mse = [\n",
    "        mean_squared_error(Y_test[:, i], Y_pred[:, i])\n",
    "        for i in range(Y.shape[1])\n",
    "    ]\n",
    "\n",
    "    # True-best and true-second indices per example\n",
    "    sorted_true_idxs = np.argsort(Y_test, axis=1)[:, ::-1]  # descending by true score\n",
    "    true_best    = sorted_true_idxs[:, 0]\n",
    "    true_second  = sorted_true_idxs[:, 1]\n",
    "\n",
    "    # Model's top-1 prediction index per example\n",
    "    pred_best = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "    # Top-1 Accuracy\n",
    "    top1_acc = np.mean(pred_best == true_best)\n",
    "\n",
    "    # Top-1-or-2 Accuracy:\n",
    "    #  counts as correct if pred_best matches either true_best or true_second\n",
    "    top1_or2_acc = np.mean([\n",
    "        pred_best[i] in {true_best[i], true_second[i]}\n",
    "        for i in range(len(pred_best))\n",
    "    ])\n",
    "\n",
    "    # Save & print\n",
    "    results[name] = {\n",
    "        \"MSE_per_LLM\": per_llm_mse,\n",
    "        \"Top1 Accuracy\": top1_acc,\n",
    "        \"Top1-or-2 Accuracy\": top1_or2_acc\n",
    "    }\n",
    "\n",
    "    print(f\"Per-LLM MSE:       {per_llm_mse}\")\n",
    "    print(f\"Top-1 Accuracy:    {top1_acc:.4f}\")\n",
    "    print(f\"Top-1-or-2 Accuracy: {top1_or2_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Load and prepare the test data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "# Features and labels\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "llm_keys = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"perplexity/sonar\"\n",
    "]\n",
    "y_scores_df = pd.DataFrame(\n",
    "    df_processed[\"scores\"].tolist(),\n",
    "    columns=llm_keys\n",
    ")\n",
    "Y = y_scores_df.values\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Train-test split (stratified by Source)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=0.20, stratify=category_labels, random_state=42\n",
    ")\n",
    "\n",
    "# === 2. Train the Random Forest regression model (Router) ===\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1\n",
    "))\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# === 3. Load or train MLP binary classifier ===\n",
    "with open(\"df_pairwise_v2.pkl\", \"rb\") as f:\n",
    "    df_pairwise = pickle.load(f)\n",
    "\n",
    "X_embed_cls = np.vstack(df_pairwise[\"embedding\"].values)\n",
    "encoder_cls = OneHotEncoder(sparse_output=False)\n",
    "X_cat_cls = encoder_cls.fit_transform(df_pairwise[[\"llm_A\", \"llm_B\"]])\n",
    "X_cls = np.hstack([X_embed_cls, X_cat_cls])\n",
    "y_cls = df_pairwise[\"label\"].values\n",
    "\n",
    "X_train_cls, X_val_cls, y_train_cls, y_val_cls = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=42\n",
    ")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "# === 4. Confidence-based routing logic with correct mapping ===\n",
    "true_top1 = np.argmax(Y_test, axis=1)\n",
    "sorted_preds = np.sort(Y_pred, axis=1)[:, ::-1]\n",
    "ranked_preds = np.argsort(Y_pred, axis=1)[:, ::-1]\n",
    "\n",
    "top1_idx = ranked_preds[:, 0]\n",
    "top2_idx = ranked_preds[:, 1]\n",
    "top1_score = sorted_preds[:, 0]\n",
    "top2_score = sorted_preds[:, 1]\n",
    "gap = top1_score - top2_score\n",
    "\n",
    "# Mapping from full LLM names to short names\n",
    "key_to_short = {\n",
    "    \"openai/gpt-4o\": \"gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\": \"claude\",\n",
    "    \"deepseek/deepseek-chat\": \"deepseek\",\n",
    "    \"perplexity/sonar\": \"perplexity\"\n",
    "}\n",
    "\n",
    "thresholds = np.arange(0.01, 0.21, 0.01)\n",
    "results = []\n",
    "\n",
    "for tau in thresholds:\n",
    "    routed_set_contains_best = []\n",
    "    overall_selection_correct = []\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        emb = X_test[i]\n",
    "        # Always include top1, sometimes top2 if gap < tau\n",
    "        routed_llms = [top1_idx[i]]\n",
    "        use_classifier = False\n",
    "\n",
    "        if gap[i] < tau:\n",
    "            routed_llms.append(top2_idx[i])\n",
    "            use_classifier = True\n",
    "\n",
    "        # Coverage: does the routed set include the true best LLM?\n",
    "        routed_set_contains_best.append(true_top1[i] in routed_llms)\n",
    "\n",
    "        # Final pick:\n",
    "        if not use_classifier:\n",
    "            selected_idx = top1_idx[i]\n",
    "        else:\n",
    "            llmA_idx, llmB_idx = top1_idx[i], top2_idx[i]\n",
    "            llmA_full = list(y_scores_df.columns)[llmA_idx]\n",
    "            llmB_full = list(y_scores_df.columns)[llmB_idx]\n",
    "            llmA_name = key_to_short[llmA_full]\n",
    "            llmB_name = key_to_short[llmB_full]\n",
    "            pair = pd.DataFrame([[llmA_name, llmB_name]], columns=[\"llm_A\", \"llm_B\"])\n",
    "            pair_cat = encoder_cls.transform(pair)\n",
    "            mlp_input = np.hstack([emb, pair_cat[0]])\n",
    "            prob = mlp.predict_proba([mlp_input])[0][1]\n",
    "            selected_idx = llmA_idx if prob >= 0.5 else llmB_idx\n",
    "\n",
    "        overall_selection_correct.append(selected_idx == true_top1[i])\n",
    "\n",
    "    routed_set_acc = np.mean(routed_set_contains_best)\n",
    "    overall_acc = np.mean(overall_selection_correct)\n",
    "    results.append((tau, routed_set_acc, overall_acc))\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Threshold\", \"Coverage Accuracy\", \"Overall Selection Accuracy\"]\n",
    ")\n",
    "print(results_df)\n",
    "\n",
    "# === 5. Plotting ===\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(results_df[\"Threshold\"], results_df[\"Coverage Accuracy\"], marker='o', label=\"Coverage Accuracy\")\n",
    "plt.plot(results_df[\"Threshold\"], results_df[\"Overall Selection Accuracy\"], marker='s', label=\"Overall Selection Accuracy\")\n",
    "plt.xlabel(\"Gap Threshold ($\\\\tau$)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Confidence-Based Routing: Coverage and Selection Accuracy vs. Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Imports\n",
    "# ------------------------------------------------------------\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load and preprocess data\n",
    "# ------------------------------------------------------------\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "llm_keys = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"perplexity/sonar\"\n",
    "]\n",
    "y_scores_df = pd.DataFrame(\n",
    "    df_processed[\"scores\"].tolist(),\n",
    "    columns=llm_keys\n",
    ")\n",
    "Y = y_scores_df.values\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=0.20, stratify=category_labels, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Train Random Forest Regressor (Router)\n",
    "# ------------------------------------------------------------\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1\n",
    "))\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Train or Load MLP Classifier\n",
    "# ------------------------------------------------------------\n",
    "with open(\"df_pairwise_v2.pkl\", \"rb\") as f:\n",
    "    df_pairwise = pickle.load(f)\n",
    "\n",
    "X_embed_cls = np.vstack(df_pairwise[\"embedding\"].values)\n",
    "encoder_cls = OneHotEncoder(sparse_output=False)\n",
    "X_cat_cls = encoder_cls.fit_transform(df_pairwise[[\"llm_A\", \"llm_B\"]])\n",
    "X_cls = np.hstack([X_embed_cls, X_cat_cls])\n",
    "y_cls = df_pairwise[\"label\"].values\n",
    "\n",
    "X_train_cls, X_val_cls, y_train_cls, y_val_cls = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=42\n",
    ")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Routing decisions\n",
    "# ------------------------------------------------------------\n",
    "true_top1 = np.argmax(Y_test, axis=1)\n",
    "sorted_preds = np.sort(Y_pred, axis=1)[:, ::-1]\n",
    "ranked_preds = np.argsort(Y_pred, axis=1)[:, ::-1]\n",
    "\n",
    "top1_idx = ranked_preds[:, 0]\n",
    "top2_idx = ranked_preds[:, 1]\n",
    "top1_score = sorted_preds[:, 0]\n",
    "top2_score = sorted_preds[:, 1]\n",
    "gap = top1_score - top2_score\n",
    "\n",
    "key_to_short = {\n",
    "    \"openai/gpt-4o\": \"gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\": \"claude\",\n",
    "    \"deepseek/deepseek-chat\": \"deepseek\",\n",
    "    \"perplexity/sonar\": \"perplexity\"\n",
    "}\n",
    "llm_names = list(y_scores_df.columns)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Evaluate pipeline win rate at different thresholds\n",
    "# ------------------------------------------------------------\n",
    "gap_values = [0.07, 0.10, 0.11, 0.12]\n",
    "pipeline_win_rates = {}\n",
    "\n",
    "for tau in gap_values:\n",
    "    print(f\"\\n=== Pipeline Routing with τ = {tau} ===\")\n",
    "    final_selected_indices = []\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        emb = X_test[i]\n",
    "        gap_i = gap[i]\n",
    "\n",
    "        if gap_i < tau:\n",
    "            llmA_idx, llmB_idx = top1_idx[i], top2_idx[i]\n",
    "            llmA_full = llm_names[llmA_idx]\n",
    "            llmB_full = llm_names[llmB_idx]\n",
    "            llmA_name = key_to_short[llmA_full]\n",
    "            llmB_name = key_to_short[llmB_full]\n",
    "\n",
    "            pair = pd.DataFrame([[llmA_name, llmB_name]], columns=[\"llm_A\", \"llm_B\"])\n",
    "            pair_cat = encoder_cls.transform(pair)\n",
    "            mlp_input = np.hstack([emb, pair_cat[0]])\n",
    "            prob = mlp.predict_proba([mlp_input])[0][1]\n",
    "            selected_idx = llmA_idx if prob >= 0.5 else llmB_idx\n",
    "        else:\n",
    "            selected_idx = top1_idx[i]\n",
    "\n",
    "        final_selected_indices.append(selected_idx)\n",
    "\n",
    "    final_selected_indices = np.array(final_selected_indices)\n",
    "    chosen_scores = Y_test[np.arange(len(Y_test)), final_selected_indices]\n",
    "\n",
    "    # --- Compute win rates ---\n",
    "    win_rates = []\n",
    "    for llm_idx in range(Y_test.shape[1]):\n",
    "        spec_mask = (final_selected_indices == llm_idx)\n",
    "        n_spec = np.sum(spec_mask)\n",
    "        llm_scores = Y_test[:, llm_idx]\n",
    "        better_mask = ~spec_mask & (chosen_scores > llm_scores)\n",
    "        n_better = np.sum(better_mask)\n",
    "        win_rate = (0.5 * n_spec + n_better) / len(Y_test)\n",
    "        win_rates.append(win_rate)\n",
    "\n",
    "    pipeline_win_rates[f\"τ={tau}\"] = [f\"{100 * wr:.2f}%\" for wr in win_rates]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Print Win Rate Comparison Table\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== Pipeline Win Rate Comparison by Threshold ===\")\n",
    "print(f\"{'τ':<10} {'GPT-4o':>10} {'Claude':>10} {'DeepSeek':>10} {'Perplexity':>12}\")\n",
    "for tau_label, rates in pipeline_win_rates.items():\n",
    "    print(f\"{tau_label:<10} {rates[0]:>10} {rates[1]:>10} {rates[2]:>10} {rates[3]:>12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 1. Load and prepare the test data ===\n",
    "with open(\"df_processed.pkl\", \"rb\") as f:\n",
    "    df_processed = pickle.load(f)\n",
    "\n",
    "df_raw = pd.read_csv(\"ranked_responses_final.csv\")\n",
    "prompt_to_source = (\n",
    "    df_raw[[\"Prompt\", \"Source\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"Prompt\")[\"Source\"]\n",
    ")\n",
    "df_processed[\"Source\"] = df_processed[\"prompt\"].map(prompt_to_source)\n",
    "df_processed = df_processed.dropna(subset=[\"Source\"]).reset_index(drop=True)\n",
    "\n",
    "# Features and labels\n",
    "X = np.vstack(df_processed[\"embedding\"].values)\n",
    "X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "llm_keys = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"perplexity/sonar\"\n",
    "]\n",
    "y_scores_df = pd.DataFrame(\n",
    "    df_processed[\"scores\"].tolist(),\n",
    "    columns=llm_keys\n",
    ")\n",
    "Y = y_scores_df.values\n",
    "category_labels = df_processed[\"Source\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Train-test split (stratified by Source)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_norm, Y, test_size=0.20, stratify=category_labels, random_state=42\n",
    ")\n",
    "\n",
    "# === 2. Train the Random Forest regression model (Router) ===\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_leaf=5, random_state=42, n_jobs=-1\n",
    "))\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# === 3. Load or train MLP binary classifier ===\n",
    "with open(\"df_pairwise_v2.pkl\", \"rb\") as f:\n",
    "    df_pairwise = pickle.load(f)\n",
    "\n",
    "X_embed_cls = np.vstack(df_pairwise[\"embedding\"].values)\n",
    "encoder_cls = OneHotEncoder(sparse_output=False)\n",
    "X_cat_cls = encoder_cls.fit_transform(df_pairwise[[\"llm_A\", \"llm_B\"]])\n",
    "X_cls = np.hstack([X_embed_cls, X_cat_cls])\n",
    "y_cls = df_pairwise[\"label\"].values\n",
    "\n",
    "X_train_cls, X_val_cls, y_train_cls, y_val_cls = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.2, random_state=42\n",
    ")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "# === 4. Confidence-based routing logic with correct mapping ===\n",
    "true_top1 = np.argmax(Y_test, axis=1)\n",
    "sorted_preds = np.sort(Y_pred, axis=1)[:, ::-1]\n",
    "ranked_preds = np.argsort(Y_pred, axis=1)[:, ::-1]\n",
    "\n",
    "top1_idx = ranked_preds[:, 0]\n",
    "top2_idx = ranked_preds[:, 1]\n",
    "top1_score = sorted_preds[:, 0]\n",
    "top2_score = sorted_preds[:, 1]\n",
    "gap = top1_score - top2_score\n",
    "\n",
    "key_to_short = {\n",
    "    \"openai/gpt-4o\": \"gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\": \"claude\",\n",
    "    \"deepseek/deepseek-chat\": \"deepseek\",\n",
    "    \"perplexity/sonar\": \"perplexity\"\n",
    "}\n",
    "\n",
    "thresholds = np.arange(0.01, 0.21, 0.01)\n",
    "results = []\n",
    "\n",
    "for tau in thresholds:\n",
    "    routed_set_contains_best = []\n",
    "    overall_selection_correct = []\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        emb = X_test[i]\n",
    "        routed_llms = [top1_idx[i]]\n",
    "        use_classifier = gap[i] < tau\n",
    "\n",
    "        if use_classifier:\n",
    "            routed_llms.append(top2_idx[i])\n",
    "\n",
    "        # Coverage accuracy\n",
    "        routed_set_contains_best.append(true_top1[i] in routed_llms)\n",
    "\n",
    "        # Final prediction\n",
    "        if not use_classifier:\n",
    "            selected_idx = top1_idx[i]\n",
    "        else:\n",
    "            llmA_idx, llmB_idx = top1_idx[i], top2_idx[i]\n",
    "            llmA_full = list(y_scores_df.columns)[llmA_idx]\n",
    "            llmB_full = list(y_scores_df.columns)[llmB_idx]\n",
    "            llmA_name = key_to_short[llmA_full]\n",
    "            llmB_name = key_to_short[llmB_full]\n",
    "            pair = pd.DataFrame([[llmA_name, llmB_name]], columns=[\"llm_A\", \"llm_B\"])\n",
    "            pair_cat = encoder_cls.transform(pair)\n",
    "            mlp_input = np.hstack([emb, pair_cat[0]])\n",
    "            prob = mlp.predict_proba([mlp_input])[0][1]\n",
    "            selected_idx = llmA_idx if prob >= 0.5 else llmB_idx\n",
    "\n",
    "        overall_selection_correct.append(selected_idx == true_top1[i])\n",
    "\n",
    "    routed_set_acc = np.mean(routed_set_contains_best)\n",
    "    overall_acc = np.mean(overall_selection_correct)\n",
    "    results.append((tau, routed_set_acc, overall_acc))\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Threshold\", \"Coverage Accuracy\", \"Overall Selection Accuracy\"]\n",
    ")\n",
    "print(\"\\n--- Accuracy Results ---\")\n",
    "print(results_df)\n",
    "\n",
    "# === 5. Compute classifier usage: Pr[g(p) < tau] ===\n",
    "classifier_usage = []\n",
    "\n",
    "for tau in thresholds:\n",
    "    usage_count = np.sum(gap < tau)\n",
    "    usage_rate = usage_count / len(gap)\n",
    "    classifier_usage.append((tau, usage_rate))\n",
    "\n",
    "classifier_usage_df = pd.DataFrame(\n",
    "    classifier_usage, columns=[\"Threshold\", \"Classifier Usage\"]\n",
    ")\n",
    "print(\"\\n--- Classifier usage (Pr[g < tau]) ---\")\n",
    "print(classifier_usage_df)\n",
    "\n",
    "# === 6. Plotting ===\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(results_df[\"Threshold\"], results_df[\"Coverage Accuracy\"], marker='o', label=\"Coverage Accuracy\")\n",
    "plt.plot(results_df[\"Threshold\"], results_df[\"Overall Selection Accuracy\"], marker='s', label=\"Overall Selection Accuracy\")\n",
    "plt.plot(classifier_usage_df[\"Threshold\"], classifier_usage_df[\"Classifier Usage\"], marker='^', label=\"Classifier Usage\", linestyle='--')\n",
    "plt.xlabel(\"Gap Threshold ($\\\\tau$)\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Routing Performance vs. Confidence Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
